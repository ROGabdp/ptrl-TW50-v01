# -*- coding: utf-8 -*-
"""
Pro Trader RL (Paper Strict Version)
åš´æ ¼éµç…§è«–æ–‡ "Pro Trader RL" çš„é‚è¼¯ã€çå‹µå‡½æ•¸èˆ‡åƒæ•¸è¨­å®šã€‚
è³‡æ–™é›†ï¼šå°ç£ 50 (TW50)
"""

import os
import sys
import datetime
import numpy as np
import pandas as pd
import yfinance as yf
import ta
from ta.volatility import AverageTrueRange
from ta.momentum import RSIIndicator
from ta.volume import MFIIndicator
import gymnasium as gym
import matplotlib.pyplot as plt
import torch
from tqdm import tqdm
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
import glob
import multiprocessing

# --- 0. ç’°å¢ƒèˆ‡ GPU è¨­å®š ---
def setup_environment():
    if torch.cuda.is_available():
        print(f"âœ… CUDA is available! Device: {torch.cuda.get_device_name(0)}")
        device = "cuda"
    else:
        print("âš ï¸ CUDA not available. Using CPU.")
        device = "cpu"

    PROJECT_PATH = os.getcwd()
    MODELS_PATH = os.path.join(PROJECT_PATH, 'models_paper') # å€éš”æ¨¡å‹è³‡æ–™å¤¾
    RESULTS_PATH = os.path.join(PROJECT_PATH, 'results_paper')
    DATA_PATH = os.path.join(PROJECT_PATH, 'data')

    for path in [MODELS_PATH, RESULTS_PATH, DATA_PATH]:
        if not os.path.exists(path):
            os.makedirs(path)
            
    return PROJECT_PATH, MODELS_PATH, RESULTS_PATH, DATA_PATH, device

# --- 1. è³‡æ–™ä¸‹è¼‰ (èˆ‡åŸç‰ˆç›¸åŒ) ---
def fetch_tw50_data(data_path, start_date="2010-01-01"):
    tickers = [
        "0050.TW", "^TWII",
        "2330.TW", "2317.TW", "2454.TW", "2881.TW", "2382.TW", "2308.TW", "2882.TW", "2412.TW", "2891.TW", "3711.TW",
        "2886.TW", "2884.TW", "2303.TW", "2885.TW", "3231.TW", "3034.TW", "5880.TW", "2892.TW", "3008.TW", "2357.TW",
        "2002.TW", "2890.TW", "1101.TW", "2880.TW", "2883.TW", "2887.TW", "2345.TW", "3045.TW", "5871.TW", "3037.TW",
        "2912.TW", "1216.TW", "6505.TW", "4938.TW", "5876.TW", "1303.TW", "2395.TW", "2379.TW", "1301.TW", "3017.TW",
        "1326.TW", "2603.TW", "1590.TW", "3661.TW", "2327.TW", "4904.TW", "2801.TW", "1605.TW", "1504.TW", "2207.TW"
    ]
    print(f"é–‹å§‹ä¸‹è¼‰ {len(tickers)} æª”æ¨™çš„è³‡æ–™...")
    data = yf.download(tickers, start=start_date, group_by='ticker', auto_adjust=True, threads=True, progress=False)
    clean_data = {}
    if "^TWII" in data.columns.levels[0]:
        market_index = data["^TWII"].copy()
        if isinstance(market_index.columns, pd.MultiIndex):
            market_index.columns = market_index.columns.get_level_values(0)
        clean_data["^TWII"] = market_index

    for t in tqdm(tickers):
        if t == "^TWII": continue
        try:
            df = data[t].copy()
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = df.columns.get_level_values(0)
            df = df.rename(columns={"Open": "Open", "High": "High", "Low": "Low", "Close": "Close", "Volume": "Volume"})
            df = df.dropna()
            if len(df) > 250:
                clean_data[t] = df
        except Exception as e:
            print(f"Skipping {t}: {e}")
    return clean_data, clean_data.get("^TWII")

# --- 2. ç‰¹å¾µå·¥ç¨‹ (Feature Engineering) ---
FEATURE_COLS = [
    'Norm_Close', 'Norm_Open', 'Norm_High', 'Norm_Low',
    'Norm_DC_Lower',
    'Norm_HA_Open', 'Norm_HA_High', 'Norm_HA_Low', 'Norm_HA_Close',
    'Norm_SuperTrend_1', 'Norm_SuperTrend_2',
    'Norm_RSI', 'Norm_MFI',
    'Norm_ATR_Change',
    'Norm_RS_Ratio',
    'RS_ROC_5', 'RS_ROC_10', 'RS_ROC_20', 'RS_ROC_60', 'RS_ROC_120'
]

# --- Helper Functions for Indicators ---
def calculate_heikin_ashi(df):
    ha_close = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4
    
    ha_open = [df['Open'].iloc[0]]
    for i in range(1, len(df)):
        ha_open.append((ha_open[-1] + ha_close.iloc[i-1]) / 2)
    ha_open = pd.Series(ha_open, index=df.index)
    
    ha_high = df[['High', 'Open', 'Close']].max(axis=1) # Simplified, strictly it's max(High, HA_Open, HA_Close)
    ha_high = pd.concat([df['High'], ha_open, ha_close], axis=1).max(axis=1)
    
    ha_low = pd.concat([df['Low'], ha_open, ha_close], axis=1).min(axis=1)
    
    return pd.DataFrame({
        'HA_open': ha_open,
        'HA_high': ha_high,
        'HA_low': ha_low,
        'HA_close': ha_close
    })

def calculate_supertrend(df, length=14, multiplier=3.0):
    high = df['High']
    low = df['Low']
    close = df['Close']
    
    atr = AverageTrueRange(high, low, close, window=length).average_true_range()
    atr = atr.fillna(method='bfill') # Fix: Fill initial NaNs to allow recursion
    
    hl2 = (high + low) / 2
    basic_upperband = hl2 + (multiplier * atr)
    basic_lowerband = hl2 - (multiplier * atr)
    
    final_upperband = basic_upperband.copy()
    final_lowerband = basic_lowerband.copy()
    
    trend = np.zeros(len(df))
    
    # Loop to calculate SuperTrend
    # Note: This is a simplified iterative implementation
    for i in range(1, len(df)):
        if basic_upperband.iloc[i] < final_upperband.iloc[i-1] or close.iloc[i-1] > final_upperband.iloc[i-1]:
            final_upperband.iloc[i] = basic_upperband.iloc[i]
        else:
            final_upperband.iloc[i] = final_upperband.iloc[i-1]
            
        if basic_lowerband.iloc[i] > final_lowerband.iloc[i-1] or close.iloc[i-1] < final_lowerband.iloc[i-1]:
            final_lowerband.iloc[i] = basic_lowerband.iloc[i]
        else:
            final_lowerband.iloc[i] = final_lowerband.iloc[i-1]
            
        if close.iloc[i] > final_upperband.iloc[i-1]:
            trend[i] = 1
        elif close.iloc[i] < final_lowerband.iloc[i-1]:
            trend[i] = -1
        else:
            trend[i] = trend[i-1]
            
        if trend[i] == 1:
            pass # final_upperband.iloc[i] = np.nan # Optional: Hide upper band in uptrend
        else:
            pass # final_lowerband.iloc[i] = np.nan
            
    # Return the trend line (SuperTrend value)
    st = pd.Series(np.where(trend == 1, final_lowerband, final_upperband), index=df.index)
    return pd.DataFrame({'SUPERT_': st}) # Naming to match pandas_ta style roughly

def calculate_features(df_in, benchmark_df):
    df = df_in.copy()
    # Donchian Channel
    df['DC_Upper'] = df['High'].rolling(20).max().shift(1)
    df['DC_Lower'] = df['Low'].rolling(20).min().shift(1)
    df['DC_Upper_10'] = df['High'].rolling(10).max().shift(1) # Filter Signal
    
    # Fill NA
    df['DC_Upper'] = df['DC_Upper'].fillna(method='bfill')
    df['DC_Lower'] = df['DC_Lower'].fillna(method='bfill')
    df['DC_Upper_10'] = df['DC_Upper_10'].fillna(method='bfill')

    # Indicators
    # ATR
    df['ATR'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=10).average_true_range()
    
    # RSI
    df['RSI'] = RSIIndicator(df['Close'], window=14).rsi()
    
    # MFI
    try:
        df['MFI'] = MFIIndicator(df['High'], df['Low'], df['Close'], df['Volume'], window=14).money_flow_index()
    except:
        df['MFI'] = 50.0
    
    # Heikin Ashi
    ha = calculate_heikin_ashi(df)
    df['HA_Open'] = ha['HA_open']
    df['HA_High'] = ha['HA_high']
    df['HA_Low'] = ha['HA_low']
    df['HA_Close'] = ha['HA_close']

    # SuperTrend
    st1 = calculate_supertrend(df, length=14, multiplier=2.0)
    st2 = calculate_supertrend(df, length=21, multiplier=1.0)
    df['SuperTrend_1'] = st1.iloc[:, 0]
    df['SuperTrend_2'] = st2.iloc[:, 0]

    # Normalization (Paper Logic)
    base_price = df['DC_Upper'].replace(0, np.nan).fillna(method='bfill')
    price_cols = ['Close', 'Open', 'High', 'Low', 'DC_Lower',
                  'HA_Open', 'HA_High', 'HA_Low', 'HA_Close',
                  'SuperTrend_1', 'SuperTrend_2']
    for col in price_cols:
        df[f'Norm_{col}'] = df[col] / base_price

    df['Norm_RSI'] = df['RSI'] / 100.0
    df['Norm_MFI'] = df['MFI'] / 100.0
    df['Norm_ATR_Change'] = (df['ATR'] / df['ATR'].shift(1)).fillna(1.0)

    if benchmark_df is not None:
        bench_close = benchmark_df['Close'].reindex(df.index).fillna(method='ffill')
        df['RS_Raw'] = df['Close'] / bench_close
        rs_min = df['RS_Raw'].rolling(250).min()
        rs_max = df['RS_Raw'].rolling(250).max()
        denominator = (rs_max - rs_min).replace(0, np.nan).fillna(1.0) + 1e-9
        df['Norm_RS_Ratio'] = (df['RS_Raw'] - rs_min) / denominator
        df['Norm_RS_Ratio'] = df['Norm_RS_Ratio'].fillna(0.5)
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = df['RS_Raw'].pct_change(period).fillna(0)
    else:
        df['Norm_RS_Ratio'] = 0.0
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = 0.0

    # Signals & Future Returns for Training
    df['Signal_Buy_Filter'] = (df['High'] > df['DC_Upper_10'])
    # Paper: "whether the return rate is 10% or more"
    # We calculate max return in next 20 days to check if 10% is achievable
    df['Next_20d_Max'] = df['High'].shift(-20).rolling(20).max() / df['Close'] - 1
    
    # Fix: Only drop rows where FEATURES are missing. Keep rows with missing labels (Next_20d_Max) for backtesting.
    feature_cols_to_check = [c for c in df.columns if c != 'Next_20d_Max']
    return df.dropna(subset=feature_cols_to_check)

# --- 3. RL ç’°å¢ƒå®šç¾© (Strict Paper Logic) ---

class BuyEnvPaper(gym.Env):
    """
    Buy Knowledge RL (Paper Version)
    - Reward: Discrete (+1 if return >= 10%, else 0 or +1 for correct wait)
    - Threshold: 10%
    """
    def __init__(self, data_dict, is_training=True):
        super().__init__()
        self.samples = []
        self.pos_samples = [] # Success cases (>= 10%)
        self.neg_samples = [] # Fail cases (< 10%)
        
        # æª¢æŸ¥æ¬„ä½
        sample_df = next(iter(data_dict.values()))
        missing = [c for c in FEATURE_COLS if c not in sample_df.columns]
        if missing: raise ValueError(f"Missing columns: {missing}")

        for t, df in data_dict.items():
            # Training requires labels, so we drop rows where Next_20d_Max is NaN
            df = df.dropna(subset=['Next_20d_Max'])
            signals = df[df['Signal_Buy_Filter'] == True]
            if len(signals) > 0:
                states = signals[FEATURE_COLS].values.astype(np.float32)
                future_rets = signals['Next_20d_Max'].values.astype(np.float32)
                for i in range(len(signals)):
                    sample = (states[i], future_rets[i])
                    self.samples.append(sample)
                    
                    # Class Balancing Logic
                    if future_rets[i] >= 0.10:
                        self.pos_samples.append(sample)
                    else:
                        self.neg_samples.append(sample)

        self.action_space = spaces.Discrete(2) # 0: Wait, 1: Buy
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(len(FEATURE_COLS),), dtype=np.float32)
        self.is_training = is_training
        self.idx = 0
        self.current_sample = None # To store the selected sample for the episode

    def reset(self, seed=None, options=None):
        if self.is_training:
            # Class Balancing: 50% chance to pick a positive sample
            # This forces the agent to see success cases 50% of the time,
            # preventing it from learning "Always Wait" due to low base rate.
            if np.random.rand() < 0.5 and len(self.pos_samples) > 0:
                idx = np.random.randint(0, len(self.pos_samples))
                self.current_sample = self.pos_samples[idx]
            elif len(self.neg_samples) > 0:
                idx = np.random.randint(0, len(self.neg_samples))
                self.current_sample = self.neg_samples[idx]
            else:
                # Fallback if one class is empty (unlikely)
                idx = np.random.randint(0, len(self.samples))
                self.current_sample = self.samples[idx]
        else:
            # Evaluation: Sequential (True Distribution)
            self.idx = (self.idx + 1) % len(self.samples)
            self.current_sample = self.samples[self.idx]
            
        return self.current_sample[0], {}

    def step(self, action):
        _, max_ret = self.current_sample
        reward = 0.0
        
        # Paper Logic: Threshold = 10% (0.10)
        is_success = (max_ret >= 0.10)

        if action == 1: # Buy (Predicting Success)
            if is_success:
                reward = 1.0 # Scenario 1: Predict Buy, Actual >= 10% -> +1
            else:
                reward = 0.0 # Scenario 2: Predict Buy, Actual < 10% -> 0
        else: # Wait (Predicting Fail)
            if not is_success:
                reward = 1.0 # Scenario 3: Predict Wait, Actual < 10% -> +1 (Avoided bad trade)
            else:
                reward = 0.0 # Scenario 4: Predict Wait, Actual >= 10% -> 0 (Missed opportunity)

        return self.samples[self.idx][0], reward, True, False, {}

class SellEnvPaper(gym.Env):
    """
    Sell Knowledge RL (Paper Version)
    - Observation: Features + [SellReturn] (Current/Buy_Price)
    - Reward: Relative Ranking (Max +2, Min +1) for profitable trades
    """
    def __init__(self, data_dict):
        super().__init__()
        self.episodes = []
        for t, df in data_dict.items():
            buy_indices = np.where(df['Signal_Buy_Filter'])[0]
            feature_data = df[FEATURE_COLS].values.astype(np.float32)
            close_prices = df['Close'].values.astype(np.float32)
            
            for idx in buy_indices:
                if idx + 120 < len(df):
                    episode_features = feature_data[idx : idx+120]
                    episode_prices = close_prices[idx : idx+120]
                    buy_price = episode_prices[0]
                    
                    # Pre-calculate returns for ranking logic
                    returns = episode_prices / buy_price
                    
                    self.episodes.append({
                        'features': episode_features,
                        'prices': episode_prices,
                        'returns': returns,
                        'buy_price': buy_price
                    })
                    
        self.action_space = spaces.Discrete(2) # 0: Hold, 1: Sell
        # Obs: Features + [SellReturn] (Paper mentions adding SellReturn to features)
        self.obs_dim = len(FEATURE_COLS) + 1 
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(self.obs_dim,), dtype=np.float32)

    def reset(self, seed=None, options=None):
        self.current_episode = self.episodes[np.random.randint(0, len(self.episodes))]
        self.day = 0
        return self._get_observation(), {}

    def _get_observation(self):
        market_features = self.current_episode['features'][self.day]
        current_return = self.current_episode['returns'][self.day] # SellReturn
        
        # Paper: Input state includes SellReturn
        obs = np.concatenate([market_features, [current_return]])
        return obs.astype(np.float32)

    def step(self, action):
        current_return = self.current_episode['returns'][self.day]
        reward = 0.0
        done = False
        
        # Paper Logic: 10% Threshold
        is_profitable = (current_return >= 1.10)

        if action == 1: # Sell
            if is_profitable:
                # Ranking Logic (Simplified for efficiency)
                # Paper: Rank returns >= 10%. Max=+2, Min=+1.
                # We calculate rank relative to the max return in the 120-day window
                max_ret_in_window = np.max(self.current_episode['returns'])
                if max_ret_in_window < 1.10:
                    # If whole window never reached 10%, but we are here (shouldn't happen if logic is strict, but for safety)
                    reward = 1.0 
                else:
                    # Linear interpolation between 1.0 and 2.0 based on closeness to max
                    # If current == max, reward = 2.0
                    # If current == 1.10 (min threshold), reward = 1.0
                    if max_ret_in_window == 1.10:
                        reward = 1.0
                    else:
                        ratio = (current_return - 1.10) / (max_ret_in_window - 1.10)
                        reward = 1.0 + ratio # Maps to [1.0, 2.0]
            else:
                # Predict Sell, Actual < 10% -> -1 (Paper: "If the return rate is less than 10%, -1 point")
                reward = -1.0
            done = True
            
        elif self.day >= 119: # Time's up
            # Forced sell
            if is_profitable:
                # Same ranking logic
                max_ret_in_window = np.max(self.current_episode['returns'])
                if max_ret_in_window == 1.10:
                    reward = 1.0
                else:
                    ratio = (current_return - 1.10) / (max_ret_in_window - 1.10)
                    reward = 1.0 + ratio
            else:
                reward = -1.0
            done = True
            
        else: # Hold
            if not is_profitable:
                # Predict Hold, Actual < 10% -> +0.5 (Paper: "If the return rate is less than 10%, +0.5 point")
                reward = 0.5
            else:
                # Predict Hold, Actual >= 10% -> 0 (Missed selling opportunity? Paper doesn't explicitly penalize holding profit, but implies selling is better)
                reward = 0.0
            self.day += 1

        next_obs = self._get_observation() if not done else self._get_observation()
        return next_obs, reward, done, False, {}

# --- 4. å›æ¸¬é¡åˆ¥ (Paper Strict Logic) ---
class DetailedBacktesterPaper:
    def __init__(self, data_dict, buy_model, sell_model, initial_capital=1000000):
        self.data_dict = data_dict
        self.buy_model = buy_model
        self.sell_model = sell_model
        self.cash = initial_capital
        self.inventory = {}
        self.max_positions = 10
        self.position_size_pct = 0.10
        sample_ticker = list(data_dict.keys())[0]
        self.dates = sorted(data_dict[sample_ticker].index)
        self.dates = [d for d in self.dates if d >= pd.Timestamp('2021-01-01')]
        if not self.dates:
            print("âš ï¸ Warning: No dates found for backtesting after 2021-01-01!")
        self.trade_logs = [] # æ–°å¢ï¼šäº¤æ˜“ç´€éŒ„

    def run(self):
        print(f"åŸ·è¡Œå›æ¸¬ (Paper Logic)...")
        records = []
        for current_date in tqdm(self.dates):
            # 1. è³£å‡ºæª¢æŸ¥ (Sell Knowledge + Stop Loss Rules)
            for ticker in list(self.inventory.keys()):
                info = self.inventory[ticker]
                df = self.data_dict[ticker]
                if current_date not in df.index: continue
                row = df.loc[current_date]
                price = row['Close']
                info['days_held'] += 1
                
                # Calculate Return
                current_return = price / info['cost_price']
                
                # --- Stop Loss Rules (Paper Strict) ---
                # Rule 1: Stop Loss on Dips (-10%)
                if current_return < 0.90:
                    self._sell(ticker, price, "StopLoss_Dip", current_date)
                    continue
                
                # Rule 2: Stop Loss on Sideways (20 days < 10%)
                # Note: Paper says "If return rate is 10% or less for 20 days... sell on 21st day"
                # We track max return in holding period or check daily returns? 
                # Paper implies "continuous" poor performance. We check if current return has been < 1.10 for 20 days.
                if info['days_held'] >= 20:
                    # ç°¡åŒ–å¯¦ä½œï¼šå¦‚æœæŒæœ‰è¶…é 20 å¤©ä¸”ç•¶å‰å ±é…¬ç‡ < 10%ï¼Œå‰‡è§¸ç™¼
                    # (åš´æ ¼ä¾†èªªæ‡‰è©²æª¢æŸ¥éå»é€£çºŒ 20 å¤©ï¼Œä½†é€™è£¡å‡è¨­è‹¥ç¬¬ 20 å¤©é‚„æ²’ 10% å°±ç )
                    if current_return < 1.10:
                        self._sell(ticker, price, "StopLoss_Sideways", current_date)
                        continue

                # --- Sell Knowledge RL ---
                market_features = row[FEATURE_COLS].values.astype(np.float32)
                sell_state = np.concatenate([market_features, [current_return]])
                
                # Get probabilities
                # predict returns (action, state) if deterministic=True
                # We need probabilities for the threshold logic
                obs_tensor = torch.as_tensor(sell_state).unsqueeze(0).to(self.sell_model.device)
                with torch.no_grad():
                    distribution = self.sell_model.policy.get_distribution(obs_tensor)
                    probs = distribution.distribution.probs.cpu().numpy()[0] # [prob_hold, prob_sell]
                
                prob_hold = probs[0]
                prob_sell = probs[1]
                
                # Paper Threshold: (Sell - Hold) > 0.85
                if (prob_sell - prob_hold) > 0.85:
                    self._sell(ticker, price, "AI_Signal", current_date)

            # 2. è²·å…¥æª¢æŸ¥ (Buy Knowledge)
            current_equity = 0
            for t, info in self.inventory.items():
                if current_date in self.data_dict[t].index:
                    current_equity += info['shares'] * self.data_dict[t].loc[current_date]['Close']
                else:
                    current_equity += info['shares'] * info['cost_price']
            total_asset = self.cash + current_equity

            if len(self.inventory) < self.max_positions and self.cash > 10000:
                tickers = list(self.data_dict.keys())
                np.random.shuffle(tickers)
                for ticker in tickers:
                    if ticker in self.inventory: continue
                    df = self.data_dict[ticker]
                    if current_date not in df.index: continue
                    row = df.loc[current_date]
                    if row['Signal_Buy_Filter']:
                        state = row[FEATURE_COLS].values.astype(np.float32)
                        action, _ = self.buy_model.predict(state, deterministic=True)
                        if action == 1:
                            target_amt = total_asset * self.position_size_pct
                            invest_amt = min(target_amt, self.cash)
                            if invest_amt > row['Close'] * 1000:
                                shares = int(invest_amt / row['Close'])
                                cost = shares * row['Close'] * (1 + 0.001425)
                                self.cash -= cost
                                self.inventory[ticker] = {'shares': shares, 'cost_price': row['Close'], 'days_held': 0}
                                # ç´€éŒ„è²·å…¥
                                self.trade_logs.append({
                                    'Date': current_date,
                                    'Ticker': ticker,
                                    'Action': 'Buy',
                                    'Price': row['Close'],
                                    'Reason': 'AI_Signal'
                                })
                                if len(self.inventory) >= self.max_positions: break
            
            # ç´€éŒ„
            daily_equity = 0
            held_stocks = []
            for t, info in self.inventory.items():
                if current_date in self.data_dict[t].index:
                    val = info['shares'] * self.data_dict[t].loc[current_date]['Close']
                else:
                    val = info['shares'] * info['cost_price']
                daily_equity += val
                held_stocks.append(t)
            
            records.append({
                'Date': current_date,
                'Total_Value': self.cash + daily_equity,
                'Cash': self.cash,
                'Invested_Amount': daily_equity,
                'Holdings_Count': len(held_stocks)
            })
            
        if not records:
            print("âš ï¸ Warning: No backtest records generated.")
            return pd.DataFrame(columns=['Total_Value', 'Cash', 'Invested_Amount', 'Holdings_Count']), self.trade_logs

        return pd.DataFrame(records).set_index('Date'), self.trade_logs

    def _sell(self, ticker, price, reason, date):
        info = self.inventory[ticker]
        revenue = info['shares'] * price * (1 - 0.004)
        self.cash += revenue
        del self.inventory[ticker]
        # ç´€éŒ„è³£å‡º
        self.trade_logs.append({
            'Date': date,
            'Ticker': ticker,
            'Action': 'Sell',
            'Price': price,
            'Reason': reason
        })

# --- Main Execution ---
if __name__ == "__main__":
    # 1. è¨­å®šç’°å¢ƒ
    PROJECT_PATH, MODELS_PATH, RESULTS_PATH, DATA_PATH, device = setup_environment()
    
    # 2. ä¸‹è¼‰èˆ‡è™•ç†è³‡æ–™
    raw_data_dict, market_index_df = fetch_tw50_data(DATA_PATH)
    
    print("æ­£åœ¨è¨ˆç®—ç‰¹å¾µ...")
    processed_data = {}
    bench_df = raw_data_dict.get("^TWII")
    for ticker, df in raw_data_dict.items():
        if ticker != "^TWII":
            try:
                processed_data[ticker] = calculate_features(df, bench_df)
            except Exception as e:
                print(f"Error processing {ticker}: {e}")
    
    # 3. æº–å‚™è¨“ç·´ç’°å¢ƒ (Vectorized)
    train_data = {k: v for k, v in processed_data.items() if k != "0050.TW"}
    
    # Determine number of CPUs
    n_cpu = multiprocessing.cpu_count()
    # WinError 1455 Fix: Limit n_envs to avoid OOM on Windows
    # Each env copies data and loads PyTorch, consuming significant RAM.
    # Safe limit: 4 to 6.
    n_envs = min(14, max(1, n_cpu - 1)) 
    print(f"Detected {n_cpu} CPUs. Using {n_envs} environments for training to avoid memory issues.")
    
    # Create Vectorized Environments
    # Note: We pass the class and kwargs to make_vec_env
    buy_env = make_vec_env(BuyEnvPaper, n_envs=n_envs, seed=0, vec_env_cls=SubprocVecEnv, env_kwargs={'data_dict': train_data, 'is_training': True})
    # Sell Env usually doesn't need heavy parallelization if it's just for training the sell agent which is faster, 
    # but we can vectorize it too for consistency and speed.
    sell_env = make_vec_env(SellEnvPaper, n_envs=n_envs, seed=0, vec_env_cls=SubprocVecEnv, env_kwargs={'data_dict': train_data})
    
    # Eval env should remain single for accurate evaluation
    # Eval env should remain single for accurate evaluation, but use SubprocVecEnv to match training env type
    eval_env = make_vec_env(BuyEnvPaper, n_envs=1, seed=0, vec_env_cls=SubprocVecEnv, env_kwargs={'data_dict': train_data, 'is_training': True})
    eval_sell_env = make_vec_env(SellEnvPaper, n_envs=1, seed=0, vec_env_cls=SubprocVecEnv, env_kwargs={'data_dict': train_data})
    
    # 4. è¨­å®š PPO åƒæ•¸ (Optimized for Speed)
    # Paper: "3 hidden layers" -> net_arch=[64, 64, 64] (approx)
    # Optimization: Increased batch_size and n_steps for GPU efficiency
    ppo_params = {
        "learning_rate": 0.0001,
        "n_steps": 2048 // n_envs, # Adjust n_steps so total buffer size is similar or larger
        "batch_size": 512, # Increased from 64 to 512 for GPU efficiency
        "ent_coef": 0.01, # Strict Paper Value
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "clip_range": 0.2,
        "vf_coef": 0.5,
        "verbose": 1,
        "device": device,
        "policy_kwargs": dict(net_arch=[64, 64, 64]) # 3 Hidden Layers
    }
    
    # Adjust n_steps to be at least some reasonable number
    if ppo_params["n_steps"] < 128:
        ppo_params["n_steps"] = 128
    
    # 5. è¨“ç·´ Buy Agent
    print("\n=== æª¢æŸ¥ Buy Agent æ¨¡å‹ ===")
    
    # ... (Buy Agent Params) ...
    TOTAL_TIMESTEPS_BUY = 2_000_000
    TOTAL_TIMESTEPS_SELL = 1_000_000
    print(f"è¨“ç·´æ­¥æ•¸è¨­å®š: Buy={TOTAL_TIMESTEPS_BUY}, Sell={TOTAL_TIMESTEPS_SELL}")
    print(f"(è¨»ï¼šè«–æ–‡åŸå§‹è¨­å®šç´„ç‚º Buy=12.2M, Sell=9.5M)")

    # Separate Log Paths
    BUY_RESULTS_PATH = os.path.join(RESULTS_PATH, "buy")
    SELL_RESULTS_PATH = os.path.join(RESULTS_PATH, "sell")
    for p in [BUY_RESULTS_PATH, SELL_RESULTS_PATH]:
        if not os.path.exists(p):
            os.makedirs(p)

    checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=MODELS_PATH, name_prefix="ppo_buy_paper")
    eval_callback = EvalCallback(eval_env, best_model_save_path=os.path.join(MODELS_PATH, "best_buy_paper"),
                                 log_path=BUY_RESULTS_PATH, eval_freq=2000, n_eval_episodes=100, deterministic=True, render=False)
    callbacks = CallbackList([checkpoint_callback, eval_callback])
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´å¥½çš„æ¨¡å‹
    final_buy_path = os.path.join(MODELS_PATH, "ppo_buy_paper_final.zip")
    best_buy_path = os.path.join(MODELS_PATH, "best_buy_paper", "best_model.zip")
    
    buy_model = None
    should_train_buy = True
    
    if os.path.exists(final_buy_path) or os.path.exists(best_buy_path):
        print(f"\nâš ï¸ åµæ¸¬åˆ°å·²å­˜åœ¨çš„ Buy Agent æ¨¡å‹ï¼")
        print(f"è·¯å¾‘: {final_buy_path if os.path.exists(final_buy_path) else best_buy_path}")
        print("è«‹é¸æ“‡æ“ä½œ:")
        print("1. ä½¿ç”¨ç¾æœ‰æ¨¡å‹ (è·³éè¨“ç·´)")
        print("2. æ¥çºŒå…ˆå‰ä¸­æ–·çš„è¨“ç·´ (Resume)")
        print("3. å°‡æ¨¡å‹åˆªé™¤ï¼Œé‡æ–°é–‹å§‹è¨“ç·´ (Restart)")
        
        choice = input("è«‹è¼¸å…¥é¸é … (1/2/3): ").strip()
        
        if choice == '1':
            print("âœ… é¸æ“‡ä½¿ç”¨ç¾æœ‰æ¨¡å‹ï¼Œè·³éè¨“ç·´...")
            if os.path.exists(best_buy_path):
                buy_model = PPO.load(best_buy_path, device=device)
                print(f"å·²è¼‰å…¥æœ€ä½³æ¨¡å‹: {best_buy_path}")
            else:
                buy_model = PPO.load(final_buy_path, device=device)
                print(f"å·²è¼‰å…¥æœ€çµ‚æ¨¡å‹: {final_buy_path}")
            should_train_buy = False
            
        elif choice == '2':
            print("ğŸ”„ é¸æ“‡æ¥çºŒè¨“ç·´...")
            # Search for latest checkpoint
            ckpt_files = glob.glob(os.path.join(MODELS_PATH, "ppo_buy_paper_*_steps.zip"))
            latest_ckpt = None
            if ckpt_files:
                # Extract step count and find max
                latest_ckpt = max(ckpt_files, key=lambda x: int(x.split('_')[-2]))
            
            if latest_ckpt:
                buy_model = PPO.load(latest_ckpt, env=buy_env, device=device)
                print(f"å·²è¼‰å…¥æœ€æ–°å­˜æª”æº–å‚™æ¥çºŒè¨“ç·´: {latest_ckpt}")
            elif os.path.exists(best_buy_path):
                buy_model = PPO.load(best_buy_path, env=buy_env, device=device)
                print(f"âš ï¸ æ‰¾ä¸åˆ° checkpointï¼Œå·²è¼‰å…¥æœ€ä½³æ¨¡å‹: {best_buy_path}")
            else:
                buy_model = PPO.load(final_buy_path, env=buy_env, device=device)
                print(f"å·²è¼‰å…¥æœ€çµ‚æ¨¡å‹æº–å‚™æ¥çºŒè¨“ç·´: {final_buy_path}")
            should_train_buy = True
            
        elif choice == '3':
            print("ğŸ—‘ï¸ é¸æ“‡åˆªé™¤èˆŠæ¨¡å‹ä¸¦é‡æ–°è¨“ç·´...")
            # å˜—è©¦åˆªé™¤èˆŠæª”æ¡ˆ
            for p in [final_buy_path, best_buy_path]:
                if os.path.exists(p):
                    try:
                        os.remove(p)
                        print(f"å·²åˆªé™¤: {p}")
                    except OSError as e:
                        print(f"ç„¡æ³•åˆªé™¤ {p}: {e}")
            
            # Fix: Also delete intermediate checkpoints
            ckpt_pattern = os.path.join(MODELS_PATH, "ppo_buy_paper_*_steps.zip")
            for p in glob.glob(ckpt_pattern):
                try:
                    os.remove(p)
                    print(f"å·²åˆªé™¤ Checkpoint: {p}")
                except OSError as e:
                    print(f"ç„¡æ³•åˆªé™¤ Checkpoint {p}: {e}")

            buy_model = None
            should_train_buy = True
        else:
            print("âš ï¸ è¼¸å…¥ç„¡æ•ˆï¼Œé è¨­ç‚ºä½¿ç”¨ç¾æœ‰æ¨¡å‹ (è·³éè¨“ç·´)...")
            if os.path.exists(best_buy_path):
                buy_model = PPO.load(best_buy_path, device=device)
            else:
                buy_model = PPO.load(final_buy_path, device=device)
            should_train_buy = False

    if should_train_buy:
        if buy_model is None:
            print("ğŸš€ é–‹å§‹å…¨æ–°è¨“ç·´ Buy Agent (Paper Logic)...")
            buy_model = PPO("MlpPolicy", buy_env, **ppo_params)
        else:
            print("ğŸš€ æ¥çºŒè¨“ç·´ Buy Agent (Paper Logic)...")
            
        buy_model.learn(total_timesteps=TOTAL_TIMESTEPS_BUY, callback=callbacks, reset_num_timesteps=False)
        buy_model.save(os.path.join(MODELS_PATH, "ppo_buy_paper_final"))
    
    # 6. è¨“ç·´ Sell Agent
    print("\n=== æª¢æŸ¥ Sell Agent æ¨¡å‹ ===")
    final_sell_path = os.path.join(MODELS_PATH, "ppo_sell_paper_final.zip")
    best_sell_path = os.path.join(MODELS_PATH, "best_sell_paper", "best_model.zip")
    
    sell_model = None
    should_train_sell = True
    
    # Check for checkpoints
    sell_ckpt_files = glob.glob(os.path.join(MODELS_PATH, "ppo_sell_paper_*_steps.zip"))
    has_sell_model = os.path.exists(final_sell_path) or os.path.exists(best_sell_path) or len(sell_ckpt_files) > 0
    
    if has_sell_model:
        print(f"\nâš ï¸ åµæ¸¬åˆ°å·²å­˜åœ¨çš„ Sell Agent æ¨¡å‹ï¼")
        if os.path.exists(best_sell_path):
             print(f"ç™¼ç¾æœ€ä½³æ¨¡å‹: {best_sell_path}")
        if os.path.exists(final_sell_path):
             print(f"ç™¼ç¾æœ€çµ‚æ¨¡å‹: {final_sell_path}")
        elif sell_ckpt_files:
             print(f"ç™¼ç¾ {len(sell_ckpt_files)} å€‹ Checkpoints")

        print("è«‹é¸æ“‡æ“ä½œ:")
        print("1. ä½¿ç”¨ç¾æœ‰æ¨¡å‹ (è·³éè¨“ç·´)")
        print("2. æ¥çºŒå…ˆå‰ä¸­æ–·çš„è¨“ç·´ (Resume)")
        print("3. å°‡æ¨¡å‹åˆªé™¤ï¼Œé‡æ–°é–‹å§‹è¨“ç·´ (Restart)")
        
        choice = input("è«‹è¼¸å…¥é¸é … (1/2/3): ").strip()
        
        if choice == '1':
            print("âœ… é¸æ“‡ä½¿ç”¨ç¾æœ‰æ¨¡å‹ï¼Œè·³éè¨“ç·´...")
            if os.path.exists(best_sell_path):
                sell_model = PPO.load(best_sell_path, device=device)
                print(f"å·²è¼‰å…¥æœ€ä½³æ¨¡å‹: {best_sell_path}")
            elif os.path.exists(final_sell_path):
                sell_model = PPO.load(final_sell_path, device=device)
                print(f"å·²è¼‰å…¥æœ€çµ‚æ¨¡å‹: {final_sell_path}")
            elif sell_ckpt_files:
                 latest_ckpt = max(sell_ckpt_files, key=lambda x: int(x.split('_')[-2]))
                 sell_model = PPO.load(latest_ckpt, device=device)
                 print(f"å·²è¼‰å…¥æœ€æ–° Checkpoint: {latest_ckpt}")
            should_train_sell = False
            
        elif choice == '2':
            print("ğŸ”„ é¸æ“‡æ¥çºŒè¨“ç·´...")
            latest_ckpt = None
            if sell_ckpt_files:
                latest_ckpt = max(sell_ckpt_files, key=lambda x: int(x.split('_')[-2]))
            
            if latest_ckpt:
                sell_model = PPO.load(latest_ckpt, env=sell_env, device=device)
                print(f"å·²è¼‰å…¥æœ€æ–°å­˜æª”æº–å‚™æ¥çºŒè¨“ç·´: {latest_ckpt}")
            elif os.path.exists(best_sell_path):
                sell_model = PPO.load(best_sell_path, env=sell_env, device=device)
                print(f"âš ï¸ æ‰¾ä¸åˆ° checkpointï¼Œå·²è¼‰å…¥æœ€ä½³æ¨¡å‹: {best_sell_path}")
            elif os.path.exists(final_sell_path):
                sell_model = PPO.load(final_sell_path, env=sell_env, device=device)
                print(f"å·²è¼‰å…¥æœ€çµ‚æ¨¡å‹æº–å‚™æ¥çºŒè¨“ç·´: {final_sell_path}")
            else:
                 print("âš ï¸ æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹å¯ä¾›æ¥çºŒï¼Œå°‡é‡æ–°é–‹å§‹è¨“ç·´ã€‚")
                 sell_model = None
            should_train_sell = True

        elif choice == '3':
            print("ğŸ—‘ï¸ é¸æ“‡åˆªé™¤èˆŠæ¨¡å‹ä¸¦é‡æ–°è¨“ç·´...")
            for p in [final_sell_path, best_sell_path]:
                if os.path.exists(p):
                    try:
                        os.remove(p)
                        print(f"å·²åˆªé™¤: {p}")
                    except OSError as e:
                        print(f"ç„¡æ³•åˆªé™¤ {p}: {e}")
            
            for p in sell_ckpt_files:
                try:
                    os.remove(p)
                    print(f"å·²åˆªé™¤ Checkpoint: {p}")
                except OSError as e:
                    print(f"ç„¡æ³•åˆªé™¤ Checkpoint {p}: {e}")

            sell_model = None
            should_train_sell = True
        else:
            print("âš ï¸ è¼¸å…¥ç„¡æ•ˆï¼Œé è¨­ç‚ºä½¿ç”¨ç¾æœ‰æ¨¡å‹ (è·³éè¨“ç·´)...")
            if os.path.exists(best_sell_path):
                sell_model = PPO.load(best_sell_path, device=device)
            elif os.path.exists(final_sell_path):
                sell_model = PPO.load(final_sell_path, device=device)
            elif sell_ckpt_files:
                 latest_ckpt = max(sell_ckpt_files, key=lambda x: int(x.split('_')[-2]))
                 sell_model = PPO.load(latest_ckpt, device=device)
            should_train_sell = False

    if should_train_sell:
        if sell_model is None:
            print("ğŸš€ é–‹å§‹å…¨æ–°è¨“ç·´ Sell Agent (Paper Logic)...")
            sell_model = PPO("MlpPolicy", sell_env, **ppo_params)
        else:
            print("ğŸš€ æ¥çºŒè¨“ç·´ Sell Agent (Paper Logic)...")
        
        # Add CheckpointCallback for Sell Agent
        sell_checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=MODELS_PATH, name_prefix="ppo_sell_paper")
        sell_eval_callback = EvalCallback(eval_sell_env, best_model_save_path=os.path.join(MODELS_PATH, "best_sell_paper"),
                                     log_path=SELL_RESULTS_PATH, eval_freq=2000, n_eval_episodes=100, deterministic=True, render=False)
        sell_callbacks = CallbackList([sell_checkpoint_callback, sell_eval_callback])
        
        sell_model.learn(total_timesteps=TOTAL_TIMESTEPS_SELL, callback=sell_callbacks, reset_num_timesteps=False)
        sell_model.save(os.path.join(MODELS_PATH, "ppo_sell_paper_final"))
    
    # 7. åŸ·è¡Œå›æ¸¬
    print("\n=== åŸ·è¡Œå›æ¸¬ (Paper Logic) ===")
    best_buy_path = os.path.join(MODELS_PATH, "best_buy_paper", "best_model.zip")
    if os.path.exists(best_buy_path):
        buy_model = PPO.load(best_buy_path, device=device)
    else:
        buy_model = PPO.load(os.path.join(MODELS_PATH, "ppo_buy_paper_final"), device=device)
    
    best_sell_path = os.path.join(MODELS_PATH, "best_sell_paper", "best_model.zip")
    if os.path.exists(best_sell_path):
        sell_model = PPO.load(best_sell_path, device=device)
        print(f"å›æ¸¬ä½¿ç”¨æœ€ä½³ Sell Model: {best_sell_path}")
    else:
        sell_model = PPO.load(os.path.join(MODELS_PATH, "ppo_sell_paper_final"), device=device)
        print(f"å›æ¸¬ä½¿ç”¨æœ€çµ‚ Sell Model")
    
    stock_data_only = {k: v for k, v in processed_data.items() if k != "^TWII" and k != "0050.TW"}
    analyzer = DetailedBacktesterPaper(stock_data_only, buy_model, sell_model)
    daily_stats, trade_logs = analyzer.run()
    
    # 8. è¼¸å‡ºçµæœèˆ‡ç¹ªåœ–
    print("\nå›æ¸¬å®Œæˆï¼")
    initial_val = daily_stats.iloc[0]['Total_Value']
    final_val = daily_stats.iloc[-1]['Total_Value']
    print(f"æœŸåˆè³‡é‡‘: {initial_val:.0f}")
    print(f"æœŸæœ«è³‡é‡‘: {final_val:.0f}")

    # --- åˆ—å°äº¤æ˜“ç´€éŒ„ (æ–°å¢åŠŸèƒ½) ---
    print("\n=== äº¤æ˜“ç´€éŒ„æ˜ç´° ===")
    print(f"{'æ—¥æœŸ':<12} {'ä»£è™Ÿ':<10} {'å‹•ä½œ':<6} {'åƒ¹æ ¼':<10} {'åŸå› '}")
    print("-" * 60)
    # å°‡ trade_logs è½‰ç‚º DataFrame æ–¹ä¾¿æ’åºèˆ‡é¡¯ç¤º
    if trade_logs:
        logs_df = pd.DataFrame(trade_logs)
        logs_df['Date'] = pd.to_datetime(logs_df['Date'])
        logs_df = logs_df.sort_values(by='Date')
        
        for _, row in logs_df.iterrows():
            date_str = row['Date'].strftime('%Y-%m-%d')
            ticker = row['Ticker'].replace('.TW', '')
            action = "è²·å…¥" if row['Action'] == 'Buy' else "è³£å‡º"
            price = f"{row['Price']:.2f}"
            reason = row['Reason']
            print(f"{date_str:<12} {ticker:<10} {action:<6} {price:<10} {reason}")
    else:
        print("ç„¡äº¤æ˜“ç´€éŒ„")
    print("-" * 60)
    
    # --- æº–å‚™ Benchmark (0050.TW) ---
    bench_df = processed_data.get("0050.TW")
    if bench_df is not None:
        # æˆªå–å›æ¸¬æœŸé–“çš„ Benchmark è³‡æ–™
        bench_df = bench_df.loc[daily_stats.index[0]:daily_stats.index[-1]]
        # æ­£è¦åŒ–ï¼šè®“ Benchmark èµ·å§‹è³‡é‡‘èˆ‡ AI ç›¸åŒ
        bench_norm = (bench_df['Close'] / bench_df['Close'].iloc[0]) * initial_val
    else:
        bench_norm = None
        print("âš ï¸ ç„¡æ³•å–å¾— 0050.TW è³‡æ–™ä½œç‚º Benchmark")

    # --- ç¹ªåœ– ---
    plt.figure(figsize=(14, 8))
    
    # 1. ç¹ªè£½ AI ç¸¾æ•ˆ
    plt.plot(daily_stats.index, daily_stats['Total_Value'], label='AI Portfolio', color='blue', linewidth=2)
    
    # 2. ç¹ªè£½ Benchmark
    if bench_norm is not None:
        plt.plot(bench_norm.index, bench_norm, label='Benchmark (0050.TW)', color='gray', linestyle='--', alpha=0.8)
    
    # 3. æ¨™è¨»è²·è³£é»
    # ç‚ºäº†é¿å…åœ–è¡¨éæ–¼é›œäº‚ï¼Œæˆ‘å€‘åªæ¨™è¨»é»ï¼Œä¸¦ç”¨ä¸åŒé¡è‰²å€åˆ†
    buy_logs = [log for log in trade_logs if log['Action'] == 'Buy']
    sell_logs = [log for log in trade_logs if log['Action'] == 'Sell']
    
    # æå–è²·å…¥é»çš„æ—¥æœŸèˆ‡å°æ‡‰çš„ Portfolio Value (ç‚ºäº†æ¨™åœ¨æ›²ç·šä¸Š)
    buy_dates = [log['Date'] for log in buy_logs]
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¥æœŸå°æ‡‰çš„ Value
    buy_values = [daily_stats.loc[d]['Total_Value'] for d in buy_dates if d in daily_stats.index]
    buy_dates_filtered = [d for d in buy_dates if d in daily_stats.index] # ç¢ºä¿å°é½Š
    
    sell_dates = [log['Date'] for log in sell_logs]
    sell_values = [daily_stats.loc[d]['Total_Value'] for d in sell_dates if d in daily_stats.index]
    sell_dates_filtered = [d for d in sell_dates if d in daily_stats.index]

    # ä¿®æ”¹é¡è‰²ï¼šè²·å…¥=ç´…è‰² (Red), è³£å‡º=ç¶ è‰² (Green)
    plt.scatter(buy_dates_filtered, buy_values, marker='^', color='red', s=50, label='Buy Signal', zorder=5)
    plt.scatter(sell_dates_filtered, sell_values, marker='v', color='green', s=50, label='Sell Signal', zorder=5)

    # 4. æ¨™è¨»è‚¡ç¥¨ä»£è™Ÿ (é¸å¡«ï¼Œé¿å…é‡ç–Šéå¤šåªæ¨™è¨»å‰å¹¾å€‹æˆ–é–“éš”æ¨™è¨»)
    # é€™è£¡ç¤ºç¯„æ¨™è¨»ï¼šåªæ¨™è¨»è²·å…¥é»çš„è‚¡ç¥¨ä»£è™Ÿ
    for i, log in enumerate(buy_logs):
        if log['Date'] in daily_stats.index:
             # ç°¡å–®é˜²æ­¢æ–‡å­—é‡ç–Šï¼šç¨å¾®éŒ¯é–‹ y è»¸
             y_val = daily_stats.loc[log['Date']]['Total_Value']
             plt.text(log['Date'], y_val * 1.02, log['Ticker'].replace('.TW', ''), fontsize=8, color='red', alpha=0.7)

    plt.title('Pro Trader RL (Paper Logic) vs 0050.TW Benchmark')
    plt.xlabel('Date')
    plt.ylabel('Portfolio Value (TWD)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    save_path = os.path.join(RESULTS_PATH, "paper_performance_enhanced.png")
    plt.savefig(save_path)
    print(f"å¢å¼·ç‰ˆåœ–è¡¨å·²å„²å­˜è‡³: {save_path}")
